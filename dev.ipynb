{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhammed/anaconda3/envs/conda_vid_eval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: You are provided the following series of thirteen frames from a 0:00:13 [H:MM:SS] video.\n",
      "\n",
      "Frame from 00:00:\n",
      "Frame from 00:01:\n",
      "Frame from 00:02:\n",
      "Frame from 00:03:\n",
      "Frame from 00:04:\n",
      "Frame from 00:05:\n",
      "Frame from 00:06:\n",
      "Frame from 00:07:\n",
      "Frame from 00:08:\n",
      "Frame from 00:09:\n",
      "Frame from 00:10:\n",
      "Frame from 00:12:\n",
      "Frame from 00:13:\n",
      "\n",
      "Can you describe this image?\n",
      "Assistant: The video consists of a series of images, each with a different background color and a person standing against it. The person is wearing a dark sweater and has long hair. The background colors change from blue to gray to white. The person is seen gesturing with their hands, possibly explaining or discussing something. The text\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"url\": \"/home/mhammed/Downloads/videoplayback.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe this video?\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(generated_texts[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User: You are provided the following series of sixty-four frames from a 0:32:28 [H:MM:SS] video.\\n\\nFrame from 00:01:\\nFrame from 00:31:\\nFrame from 01:02:\\nFrame from 01:33:\\nFrame from 02:04:\\nFrame from 02:35:\\nFrame from 03:06:\\nFrame from 03:37:\\nFrame from 04:08:\\nFrame from 04:39:\\nFrame from 05:09:\\nFrame from 05:40:\\nFrame from 06:11:\\nFrame from 06:42:\\nFrame from 07:13:\\nFrame from 07:44:\\nFrame from 08:15:\\nFrame from 08:46:\\nFrame from 09:17:\\nFrame from 09:47:\\nFrame from 10:18:\\nFrame from 10:49:\\nFrame from 11:20:\\nFrame from 11:51:\\nFrame from 12:22:\\nFrame from 12:53:\\nFrame from 13:24:\\nFrame from 13:55:\\nFrame from 14:25:\\nFrame from 14:56:\\nFrame from 15:27:\\nFrame from 15:58:\\nFrame from 16:29:\\nFrame from 17:00:\\nFrame from 17:31:\\nFrame from 18:02:\\nFrame from 18:33:\\nFrame from 19:04:\\nFrame from 19:34:\\nFrame from 20:05:\\nFrame from 20:36:\\nFrame from 21:07:\\nFrame from 21:38:\\nFrame from 22:09:\\nFrame from 22:40:\\nFrame from 23:11:\\nFrame from 23:42:\\nFrame from 24:12:\\nFrame from 24:43:\\nFrame from 25:14:\\nFrame from 25:45:\\nFrame from 26:16:\\nFrame from 26:47:\\nFrame from 27:18:\\nFrame from 27:49:\\nFrame from 28:20:\\nFrame from 28:50:\\nFrame from 29:21:\\nFrame from 29:52:\\nFrame from 30:23:\\nFrame from 30:54:\\nFrame from 31:25:\\nFrame from 31:56:\\nFrame from 32:27:\\n\\nCan you describe this video? Be very verbose and detailed.\\nAssistant: The video showcases a series of interactions with a computer screen, starting with a web browser displaying a webpage titled \"How to get started with LeetCode.\" The user navigates through the site, interacting with various elements such as tabs, buttons, and text fields. The focus then shifts to a code editor, where the user begins writing code, and the screen transitions to a different interface, possibly a chat or messaging platform, where the user engages in a conversation. The narrative continues with the user interacting with a virtual communication skills trainer, navigating through various screens and engaging in a dialogue. The video concludes with a close-up of a webpage titled \"no microphone found, how to solve this? #6446,\" indicating a focus on troubleshooting issues related to audio input.']\n",
      "---\n",
      "User: You are provided the following series of sixty-four frames from a 0:32:28 [H:MM:SS] video.\n",
      "\n",
      "Frame from 00:01:\n",
      "Frame from 00:31:\n",
      "Frame from 01:02:\n",
      "Frame from 01:33:\n",
      "Frame from 02:04:\n",
      "Frame from 02:35:\n",
      "Frame from 03:06:\n",
      "Frame from 03:37:\n",
      "Frame from 04:08:\n",
      "Frame from 04:39:\n",
      "Frame from 05:09:\n",
      "Frame from 05:40:\n",
      "Frame from 06:11:\n",
      "Frame from 06:42:\n",
      "Frame from 07:13:\n",
      "Frame from 07:44:\n",
      "Frame from 08:15:\n",
      "Frame from 08:46:\n",
      "Frame from 09:17:\n",
      "Frame from 09:47:\n",
      "Frame from 10:18:\n",
      "Frame from 10:49:\n",
      "Frame from 11:20:\n",
      "Frame from 11:51:\n",
      "Frame from 12:22:\n",
      "Frame from 12:53:\n",
      "Frame from 13:24:\n",
      "Frame from 13:55:\n",
      "Frame from 14:25:\n",
      "Frame from 14:56:\n",
      "Frame from 15:27:\n",
      "Frame from 15:58:\n",
      "Frame from 16:29:\n",
      "Frame from 17:00:\n",
      "Frame from 17:31:\n",
      "Frame from 18:02:\n",
      "Frame from 18:33:\n",
      "Frame from 19:04:\n",
      "Frame from 19:34:\n",
      "Frame from 20:05:\n",
      "Frame from 20:36:\n",
      "Frame from 21:07:\n",
      "Frame from 21:38:\n",
      "Frame from 22:09:\n",
      "Frame from 22:40:\n",
      "Frame from 23:11:\n",
      "Frame from 23:42:\n",
      "Frame from 24:12:\n",
      "Frame from 24:43:\n",
      "Frame from 25:14:\n",
      "Frame from 25:45:\n",
      "Frame from 26:16:\n",
      "Frame from 26:47:\n",
      "Frame from 27:18:\n",
      "Frame from 27:49:\n",
      "Frame from 28:20:\n",
      "Frame from 28:50:\n",
      "Frame from 29:21:\n",
      "Frame from 29:52:\n",
      "Frame from 30:23:\n",
      "Frame from 30:54:\n",
      "Frame from 31:25:\n",
      "Frame from 31:56:\n",
      "Frame from 32:27:\n",
      "\n",
      "Can you describe this video? Be very verbose and detailed.\n",
      "Assistant: The video showcases a series of interactions with a computer screen, starting with a web browser displaying a webpage titled \"How to get started with LeetCode.\" The user navigates through the site, interacting with various elements such as tabs, buttons, and text fields. The focus then shifts to a code editor, where the user begins writing code, and the screen transitions to a different interface, possibly a chat or messaging platform, where the user engages in a conversation. The narrative continues with the user interacting with a virtual communication skills trainer, navigating through various screens and engaging in a dialogue. The video concludes with a close-up of a webpage titled \"no microphone found, how to solve this? #6446,\" indicating a focus on troubleshooting issues related to audio input.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"url\": \"/home/mhammed/Downloads/AI Verbal Communication Skills Trainer Overview.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe this video? Be very verbose and detailed.\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=256)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(generated_texts)\n",
    "for text in generated_texts:\n",
    "    print(\"---\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: You are provided the following series of sixty-four frames from a 1:23:34 [H:MM:SS] video.\n",
      "\n",
      "Frame from 00:00:\n",
      "Frame from 01:20:\n",
      "Frame from 02:40:\n",
      "Frame from 03:59:\n",
      "Frame from 05:19:\n",
      "Frame from 06:38:\n",
      "Frame from 07:58:\n",
      "Frame from 09:17:\n",
      "Frame from 10:37:\n",
      "Frame from 11:57:\n",
      "Frame from 13:16:\n",
      "Frame from 14:36:\n",
      "Frame from 15:55:\n",
      "Frame from 17:15:\n",
      "Frame from 18:34:\n",
      "Frame from 19:54:\n",
      "Frame from 21:13:\n",
      "Frame from 22:33:\n",
      "Frame from 23:53:\n",
      "Frame from 25:12:\n",
      "Frame from 26:32:\n",
      "Frame from 27:51:\n",
      "Frame from 29:11:\n",
      "Frame from 30:30:\n",
      "Frame from 31:50:\n",
      "Frame from 33:10:\n",
      "Frame from 34:29:\n",
      "Frame from 35:49:\n",
      "Frame from 37:08:\n",
      "Frame from 38:28:\n",
      "Frame from 39:47:\n",
      "Frame from 41:07:\n",
      "Frame from 42:27:\n",
      "Frame from 43:46:\n",
      "Frame from 45:06:\n",
      "Frame from 46:25:\n",
      "Frame from 47:45:\n",
      "Frame from 49:04:\n",
      "Frame from 50:24:\n",
      "Frame from 51:44:\n",
      "Frame from 53:03:\n",
      "Frame from 54:23:\n",
      "Frame from 55:42:\n",
      "Frame from 57:02:\n",
      "Frame from 58:21:\n",
      "Frame from 59:41:\n",
      "Frame from 61:00:\n",
      "Frame from 62:20:\n",
      "Frame from 63:40:\n",
      "Frame from 64:59:\n",
      "Frame from 66:19:\n",
      "Frame from 67:38:\n",
      "Frame from 68:58:\n",
      "Frame from 70:17:\n",
      "Frame from 71:37:\n",
      "Frame from 72:57:\n",
      "Frame from 74:16:\n",
      "Frame from 75:36:\n",
      "Frame from 76:55:\n",
      "Frame from 78:15:\n",
      "Frame from 79:34:\n",
      "Frame from 80:54:\n",
      "Frame from 82:14:\n",
      "Frame from 83:33:\n",
      "\n",
      "Can you describe this video? Be very verbose and detailed.\n",
      "Assistant: The video starts with a black screen that transitions to a scene featuring a person in a red beanie and a black shirt, standing in front of a wall adorned with graffiti and posters. The scene then shifts to a person in a blue hoodie and a denim jacket, sitting in a room with a boxing ring in\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"url\": \"/home/mhammed/Downloads/videoplayback (1).mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe this video? Be very verbose and detailed.\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "for text in generated_texts:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_vid_eval_3",
   "language": "python",
   "name": "conda_vid_eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
